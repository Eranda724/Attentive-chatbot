{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2340a6-2656-4f28-a600-a53028039fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Requirement already satisfied: tensorflow-datasets==1.2.0 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.1.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (24.2.0)\n",
      "Requirement already satisfied: dill in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.3.9)\n",
      "Requirement already satisfied: future in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.26.4)\n",
      "Requirement already satisfied: promise in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (3.20.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.16.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (4.66.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\erand\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tqdm->tensorflow-datasets==1.2.0) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "!pip install tensorflow-datasets==1.2.0\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e0032bb-b301-4920-a780-0f6721ed383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    \"cornell_movie_dialogs.zip\",\n",
    "    origin='https://www.kaggle.com/api/v1/datasets/download/soumikrakshit/cornell-movie-dialogs-corpus',\n",
    "    extract=True\n",
    ")\n",
    "path_to_data = os.path.join(os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_data, \"movie_lines.txt\")\n",
    "path_to_movie_conversations = os.path.join(path_to_data, \"movie_conversations.txt\")\n",
    "\n",
    "print(\"Dataset downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e5ff47e-e09f-4000-ad4b-429c7676791d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\erand\\\\.keras\\\\datasets\\\\cornell movie-dialogs corpus'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b70033b4-95a4-4e07-9e3f-f9891f24b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLE = 50000\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\",r\" \\1\",sentence)\n",
    "    sentence = re.sub(r'[\" \"]+',\" \",sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.,!]+\",\" \",sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def load_conversations():\n",
    "    id2line={}\n",
    "    with open(path_to_movie_lines, errors = \"ignore\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\",\"\").split(\" +++$+++ \")\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [],[]\n",
    "\n",
    "    with open(path_to_movie_conversations, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\",\"\").split(\" +++$+++ \")\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
    "        for i in range(len(conversation) - 1):\n",
    "            inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "            outputs.append(preprocess_sentence(id2line[conversation[i+1]]))\n",
    "\n",
    "            if len(inputs) >= MAX_SAMPLE:\n",
    "                return inputs,outputs\n",
    "        return inputs,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c0063f1-176f-4676-9730-47ac9d741301",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions , answers = load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3603f6fc-3016-4115-91b1-751d4030095a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc3da559-e5f9-41da-8717-4091b422521e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7142b84d-c3b2-4d32-bcb5-7ccd295e56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .\n",
      "A : well , i thought we d start with pronunciation , if that s okay with you .\n"
     ]
    }
   ],
   "source": [
    "print(\"Q : {}\".format(questions[0]))\n",
    "print(\"A : {}\".format(answers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "950bbc2f-d4d5-4f0f-a9e5-9e8008f3c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions+answers,target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size],[tokenizer.vocab_size+1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9949a48d-edf7-4fcb-a0e5-a56367728efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Q : [19, 7, 22, 1, 23, 1, 13, 17, 5, 16, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Q : {}\".format(tokenizer.encode(questions[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0fc94cb1-8bbe-4dca-a357-c9f0a16e3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(inputs,outputs):\n",
    "    MAX_LENGTH = 40  # Example value\n",
    "    tokenized_inputs,tokenized_outputs = [],[]\n",
    "\n",
    "    for (sentence1,sentence2) in zip(inputs,outputs):\n",
    "        sentence1 = START_TOKEN+tokenizer.encode(sentence1)+END_TOKEN\n",
    "        sentence2 = START_TOKEN+tokenizer.encode(sentence2)+END_TOKEN\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <=MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs,maxlen=MAX_LENGTH,padding=\"post\")\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs,maxlen=MAX_LENGTH,padding=\"post\")\n",
    "\n",
    "    return tokenized_inputs,tokenized_outputs\n",
    "\n",
    "questions,answers = tokenize_and_filter(questions,answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a52f1be-5360-4692-9b0c-6a6c8f376d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f64b67e-b561-447c-b130-1f43181eb75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4c00bc1-c530-4071-8981-94937d4501c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"inputs\": questions,\n",
    "        \"dec_inputs\": answers[:,:-1]\n",
    "    },\n",
    "    {\n",
    "        \"outputs:\": answers[:,1:]\n",
    "    }\n",
    "))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757227f3-8bc1-45cd-9629-6d87b0d3c712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506d1ea-fb46-4306-a309-cc502cee8974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
